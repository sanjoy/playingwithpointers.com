On CS Theory

I like mathy CS theory.  Fortunately, this meshes well with what I
hack on -- working on compilers without a solid grounding in relevant
theory is difficult at the very least.  However, I do believe some
knowledge of mathematics and general CS theory can be of significant
help in any branch of computer engineering.

Aristotle pointed out the distinction between the essential and
accidental properties [1] of an object.  That a wallet holds money is
one of its essential properties; that it lies on the table is
accidental.  If you are alien to the concept of a wallet, and you have
only seen wallets lying on tables, you may be misguided by your
experience into confusing the accidental property of "lying on a
table" as essential.  Experience, while useful, sometimes fails as an
inference tool [2], theory is the _prior knowledge_ that helps
differentiate between essence and accident.

More important than this generic observation is the specific case when
the _property_ in consideration is _software complexity_.  Some
mathematical knowledge and intuition can help differentiate between
complexity essential to the problem domain and complexity accidental
to it.  As an example, consider an image service that allows users to
tag or classify images.  An image can be tagged as a _dog_, _man_,
_woman_ or _person_. If a user tags the an image as a _man_ when
another user tagged the same as a _woman_, you want infer the tag to
be _person_ instead.  However, if one user tags an image as a _dog_
while another tags it as a _man_, you'd like to conclude that the
image is too blurry to be recognized.  What does an application that
does this look like?  How does it scale?  How do you add new kinds of
tags?  It turns out that such a structure can be represented using a
lattice [3], with the inference operation implemented as a
least-upper-bound operation -- most of the complexity is, in fact,
accidental!  You can then have domain experts create the lattice
(using tools you create for this purpose) and have the simple,
unification algorithm reading the lattice off a database, solving the
problem of creating new tags.  It gets better as you try to scale:
least-upper-bound is associative and commutative.  Given 10 million
tags for an image (on a complicated lattice) to unify, you can
arbitrarily split up the tags, send them to separate servers for
unification and then unify the results; and you know you won't go
wrong.  You could probably have come up with this yourself, but that
would take time; and you'd still not "see through" this accidental
complexity.

One of my favorite "real" examples is the use of monoids in the design
of the `diagrams` vector graphics framework [4].  Using a well-studied
algebraic structure in an engineering context gives us elegance for
free.

It goes without saying that it is possible to take the use of theory
too far.  Just because certain properties are accidental in a pedantic
sense doesn't mean looking at them from such an angle is beneficial.
For instance, the C and C++ standards have no concept of a stack or a
heap, only automatic and dynamic storage; but it would be foolish to
be _that_ pedantic.  The function call stack may be a lie, but it is
convincing enough to not be an accident.  :)

[1]: http://en.wikipedia.org/wiki/Accident_(philosophy)
[2]: http://en.wikipedia.org/wiki/Cargo_cult
[3]: http://en.wikipedia.org/wiki/Lattice\_(order)
[4]: http://cis.upenn.edu/~byorgey/pub/monoid-pearl.pdf
