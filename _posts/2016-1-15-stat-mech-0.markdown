---
layout: post
title:  "notes: statistical mechanics part 0"
permalink: theoretical-minimum-stat-mech-0.html
keywords: "physics, math"
---

I've been going through a really great set of video lectures from
Stanford: [The Theoretical
Minimum](http://theoreticalminimum.com/courses), and I figured writing
down some study notes will help me better retain what I'm learning
(and since I'm writing these down, there is no reason to not make them
public).

This post covers the material in the [Statistical
Mechanics](http://theoreticalminimum.com/courses/statistical-mechanics/2013/spring)
course, lectures one through four.

# basics

We start with a system with $$N$$ possible energy states, $$E_i$$
(with $$i$$ ranging from $$1$$ to $$N$$) [^continuous].  These energy
states (i.e. their values) have been obtained via the specific physics
of system we're studying, and are fixed as far as we're concerned.
The system is in energy state $$E_i$$ with probability $$P_i$$ and
thus $$P_i$$ forms a probability distribution with $$\sum_1^N P_i =
1$$ and $$P_i >= 0$$.  The entropy of such a system is defined to be
$$S = - \sum_1^N P_{i} \; log P_{i}$$, and is a measure of how "wide"
the probability distribution is.  $$S$$ is a function of both the
system and how much the observer knows about it (I don't yet
understand the full repercussions of this). The average energy is of
the system is, by definition, $$E = \sum_1^N P_{i} \; E_{i}$$.

[^continuous]: Prof. Susskind does not call this out very explicitly,
    but he moves between discrete and continuous probabilities as
    convenient.  As far as I understand, the former is useful when
    working within a quantum mechanical setting where energy states
    are discrete; while the latter is useful in a classical setting
    where the energy levels are continuous.  In any case, you should
    be able to interchange the discrete probability distribution with
    a continuous probability density function in this post.

We can also look at the equation for $$E$$ (average energy) in the
context of $$E$$ being a fundamental, tunable parameter of the system
in question.  In that view, corresponding to each value of $$E$$ there
is a *probability distribution* $$P(E)_i$$ such that $$E = \sum_1^N
P(E)_i \; E_{i}$$.  In typical physical systems, as $$E$$ increases,
$$P(E)_i$$ becomes "wider", and $$S$$ also increases.  This isn't
*mathematically* true -- it is easy to imagine cases where $$P(E)_i$$
gets *narrower* as $$E$$ increases, but such cases don't appear in
practice.

Temperature is a derived quantity defined as $$T = \frac{dS}{dE}$$.
It may seem weird to not give temperature a more fundamental
definition, since it is something we feel and observe "directly", but
that's pretty much the only definition that makes sense overall.
Entropy is unit-less so $$T$$ as defined here has the same units as
energy.  The temperature in Kelvin (the "laboratory temperature") can
be converted to this form by $$T = k_B T_{kelvin}$$ where $$k_B
\approx 1.38064852 \times 10^{-23} J/K$$.  From this point on,
"temperature" will mean "temperature in Joules", unless otherwise
specified.

# energy flows from higher to lower temperatures

Lets' say we have two systems $$A$$ and $$B$$ with different
temperatures $$T_A$$ and $$T_B$$, energies $$E_A$$ and $$E_B$$, and
entropy $$S_A$$ and $$S_B$$.  Without loss of generality we assume
$$T_A < T_B$$.  We know

$$dE_A + dE_B = 0 \qquad ...\; \text{first law of thermodynamics}$$

$$dS_A + dS_B > 0 \qquad ...\; \text{second law of thermodynamics}$$

Using $$dE = T \; dS$$ and some basic algebra, we can show $$dS_A >
0$$ and hence $$dE_A > 0$$, meaning that if $$T_A < T_B$$ then energy
flows from system $$B$$ into system $$A$$.

# the boltzmann distribution

Assume that we have $$N$$ sub-systems (gas molecules, say) such that
$$n_i$$ of them are in energy state $$E_i$$ (with $$i$$ running from
$$1$$ to $$k$$).  The probability of a given sub-system being in a
state with energy $$E_i$$ is therefore $$\frac{n_i}{N} = P_i$$ and the
total entropy of the system is $$S = -N \sum_1^k P_i \; log P_i$$
(entropy is additive).  We'd like to compute the probability
distribution that maximizes the entropy of the system[^entropy].  The
"independent variables" being maximized over are $$n_i$$ or $$P_i$$ --
so in other words, the question being asked is "for what values of
$$\{P_1, P_2, \ldots, P_k\}$$ (or, identically, $$\{n_1, n_2, \ldots,
n_k\}$$) will the entropy be the greatest?".  We also have some
additional constraints -- the total energy in the system is a finite
value $$E_{total}$$, and so we have $$\sum_1^k n_i \; E_i =
E_{total}$$, or alternatively, $$\sum_1^k P_i \; E_i = E$$ where $$E$$
is the average energy (per sub-system); and $$\sum_1^k P_i = 1$$, by
definition of a probability distributions.  This purely mathematical
optimization problem can be solved using [the method of Lagrange
multipliers](https://en.wikipedia.org/wiki/Lagrange_multiplier).  As
per the method of Lagrange multipliers we set $$\nabla(-\sum_1^k P_i
\; log P_i + \alpha (\sum_1^k P_i - 1) + \frac{1}{\beta}(\sum_1^k P_i
\; E_i - E))$$ $$=$$ $$0$$ with the gradient being computed with
respect to $$\{P_1, P_2, \ldots, P_k\}$$ (I chose the constant as
$$\frac{1}{\beta}$$ and not $$\beta$$ to be consistent with the
lectures).  This gives us $$P_i = e^{-1 + \alpha - E_i \beta}$$ $$=$$
$$\frac{1}{Z}e^{-E_i \beta}$$ where $$Z$$ $$=$$ $$e^{1-\alpha}$$.  The
constraints and some algebraic manipulation gives us the following
relationships:

 * The probability of a sub-system being in a specific energy state:
   $$P_i = \frac{1}{Z} e^{-E_i \beta}$$
 * The "partition function": $$Z = \sum_1^k e^{-E_i \beta}$$
 * The average energy: $$E = - \frac{\partial log Z}{\partial \beta}$$
 * The temperature: $$T = \frac{1}{\beta}$$ ($$\beta$$ is also called
   the "inverse temperature").
 * The entropy of the system: $$S = \beta E + log Z$$.

The above equations (the equation specifying the value of $$P_i$$ in
particular) form [the Boltzmann
distribution](https://en.wikipedia.org/wiki/Boltzmann_distribution).

[^entropy]: I don't fully understand the physical significance of
    maximizing the entropy of the system, but I suspect that since
    systems "evolve" by increasing their entropy, the maximum entropy
    is the "steady state" where the system is in equilibrium.  But,
    generally, entropy is a subtle concept that I'm still trying to
    wrap my head around.  Yet another interesting thing here whose
    physical significance I cannot yet grasp is that maximizing total
    entropy corresponds to discretely maximizing the number of ways
    there are to distribute the $$N$$ systems into sets of size
    $$n_i$$ (this can be proved by reasoning combinatorially, and then
    using Stirling's approximation).
